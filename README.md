# ğŸ“· Real-time Image Classifier (Android + TensorFlow Lite)

A real-time **image classification app** built in **Kotlin** using **TensorFlow Lite** and the **Camera2 API**.  
The app streams camera frames, runs them through a TFLite model, and displays predictions instantly â€” all **on-device** with no internet dependency.

---

## âœ¨ Features

- ğŸš€ **Real-time classification** with TensorFlow Lite  
- ğŸ“· **Camera2 API** integration with live preview (`AutoFitTextureView`)  
- ğŸ§  Plug-and-play support for any `.tflite` classification model  
- ğŸ” Image preprocessing utilities (`ImageUtils.kt`)  
- ğŸ§© Clean separation of **UI**, **Camera**, and **ML** components  
- ğŸ’¡ On-device inference = **fast** and **private**

---

## ğŸ“‚ Project Structure

```
app/src/main/java/com/example/realtimeapp/
â”‚â”€â”€ ML/                         # TensorFlow Lite interpreter & helpers
â”‚â”€â”€ AutoFitTextureView.kt       # Resizable TextureView for camera preview
â”‚â”€â”€ CameraConnectionFragment.kt # Handles Camera2 connection & frame callbacks
â”‚â”€â”€ ImageUtils.kt               # Frame conversion (YUV â†’ RGB, resize, normalize)
â”‚â”€â”€ MainActivity.kt             # Entry point & overlay for predictions
```

---

## ğŸ”§ Setup & Installation

### 1. Clone the repo
```bash
git clone https://github.com/<your-username>/Real-time-Image-Classifier.git
```

### 2. Add your TFLite model
- Place your model file as:  
  ```
  app/src/main/assets/model.tflite
  app/src/main/assets/labels.txt
  ```

### 3. Dependencies
In `app/build.gradle`:
```gradle
dependencies {
    implementation 'org.tensorflow:tensorflow-lite:2.14.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.14.0'          // optional
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.14.0' // optional
}
```

### 4. Run the app
- Open project in **Android Studio**  
- Connect an Android device (API 24+)  
- Run â–¶ï¸

---

## âš™ï¸ How It Works

1. `CameraConnectionFragment` streams frames via Camera2 API  
2. `ImageUtils` preprocesses frames â†’ model input size (e.g., 224Ã—224)  
3. TFLite `Interpreter` runs inference on the frame  
4. Predictions are mapped with `labels.txt` and shown in the UI  

---

## ğŸ“¸ Demo

*(Add screenshot/gif here of classification running in real-time)*  

---

## ğŸš€ Future Improvements
- Support multiple models dynamically  
- Add GPU/NNAPI acceleration toggle  
- Overlay bounding boxes for object detection models  

---

## ğŸ“ License
This project is licensed under the MIT License.  
